
1. Create EC2 ubuntu instance


chmod 600 hrnyc_kub.pem
ssh -i hrnyc_kub.pem ubuntu@18.221.99.101
sudo apt-get update && sudo apt-get upgrade -y

2. Install Kops on EC2
curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-linux-amd64
chmod +x kops-linux-amd64
sudo mv kops-linux-amd64 /usr/local/bin/kops

3. Install kubectl
curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
chmod +x ./kubectl
sudo mv ./kubectl /usr/local/bin/kubectl

4. install AWSCLI
 curl https://s3.amazonaws.com/aws-cli/awscli-bundle.zip -o awscli-bundle.zip
 apt install unzip python
 unzip awscli-bundle.zip
 #sudo apt-get install unzip - if you dont have unzip in your system
 ./awscli-bundle/install -i /usr/local/aws -b /usr/local/bin/aws


5. Create IAM role and attach to EC2 instance.
  aws configure
  default region: us-east-2


6. Create private hosted zone in AWS Route53
Create a Route53 private hosted zone in the same region as the server

sdc-server.net (Private)

7. Create S3 bucket in AWS

aws s3 mb s3://${name-of-bucket}

aws s3 mb s3://bucket.sdc.server

8. Expose environment variable:

export KOPS_STATE_STORE=s3://bucket.sdc.server

9. - Create a role and pass along permissions to the kops server; it needs to be able to spin up more servers to build the cluster?

10. Configure environment variables.

vi ~/.bashrc

export KOPS_CLUSTER_NAME=sdc-server.net
export KOPS_STATE_STORE=s3://bucket.sdc.server

source ~/.bashrc

11. Create sshkeys before creating cluster

ssh-keygen

12. Create kubernetes cluster definitions on S3 bucket

kops create cluster --state=s3://bucket.sdc.server --node-count=2 --master-count=1 --master-size=t2.micro --node-size=t2.micro --zones=us-east-2a,us-east-2b --name=sdc-server.net --dns private


13. Build cluster and validate

kops update cluster --yes
kops validate cluster

14. Shell into master

ssh admin@api.${name-of-cluster}